# -*- coding: utf-8 -*-
"""categorial_embedding_fastiai

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12ofKMyLNQheSfHpjmNBk-NncPFOP1WFy
"""

! pip install kaggle

pip install fastai==v2.7.18

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
from fastai.tabular.all import *

creds = '{"username":"navidfalah","key":"2dcec140d80bff7c7158857678855827"}'

cred_path = Path('~/.kaggle/kaggle.json').expanduser()
if not cred_path.exists():
    cred_path.parent.mkdir(exist_ok=True)
    with open(cred_path, 'w') as f:
        f.write(creds)
    cred_path.chmod(0o600)

cred_path

path = URLs.path('bluebook')
path

!kaggle competitions download -c bluebook-for-bulldozers

!unzip /content/bluebook-for-bulldozers.zip

df_train = pd.read_csv('/content/TrainAndValid.csv')

df_train.head()

df_train.columns

df_train["ProductSize"].unique()

sizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'

df_train['ProductSize'] = df_train['ProductSize'].astype('category')
df_train['ProductSize'].cat.set_categories(sizes, ordered=True)

dep_var = 'SalePrice'
df_train[dep_var] = np.log(df_train[dep_var])

df_train[dep_var]

df_train = add_datepart(df_train, 'saledate')

df_test = pd.read_csv('/content/Test.csv')
df_test = add_datepart(df_test, 'saledate')

''.join(o+"  " for o in df_train.columns if o.startswith('sale'))

procs = [Categorify, FillMissing]

cond = (df_train.saleYear<2011) | (df_train.saleMonth<10)
train_idx = np.where( cond)[0]
valid_idx = np.where(~cond)[0]
splits = (list(train_idx),list(valid_idx))

con, cat = cont_cat_split(df_train, 1, dep_var=dep_var)
to = TabularPandas(df_train, procs, cat, con, y_names=dep_var, splits=splits)

len(to.train), len(to.valid)

to.show(3)

to.items.head(3)

### now we create the decision tree

xs, y = to.train.xs, to.train.y
valid_xs, valid_y = to.valid.xs, to.valid.y

from sklearn.tree import DecisionTreeRegressor # import the DecisionTreeRegressor class from sklearn.tree

m = DecisionTreeRegressor(max_leaf_nodes=4)
m.fit(xs, y)

!pip install fastbook
from fastbook import draw_tree

draw_tree(m, xs, size=10, leaves_parallel=True, precision=2)

m = DecisionTreeRegressor()
m.fit(xs, y)

def r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6)
def m_rmse(m, xs, y): return r_mse(m.predict(xs), y)
m_rmse(m, xs, y)

m_rmse(m, valid_xs, valid_y)

m.get_n_leaves(), len(xs)

m = DecisionTreeRegressor(min_samples_leaf=25)
m.fit(xs, y)
m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)

m.get_n_leaves(), len(xs)

from sklearn.ensemble import RandomForestRegressor

def rf(xs, y, n_estimators=40, max_samples=200_000,
 max_features=0.5, min_samples_leaf=5, **kwargs):
 return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,
 max_samples=max_samples, max_features=max_features,
 min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)
m = rf(xs, y);

m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)

preds = np.stack([t.predict(valid_xs) for t in m.estimators_])
preds.shape

pred_std = preds.std(0)

pred_std[:5]

### we also needs to know how the mode is trying to predict

def rf_feat_importance(m, df):
  return pd.DataFrame({
      'cols':df.columns,
      'imp':m.feature_importances_
  }).sort_values('imp', ascending=False)

fi = rf_feat_importance(m, xs)
fi[:10]

def plot_fi(fi):
  return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)

plot_fi(fi[:30]);

### remove the low importance variables and get a good score

to_keep =  fi[fi.imp>0.005].cols
len(to_keep)

xs_imp = xs[to_keep]
valid_xs_imp = valid_xs[to_keep]

m = rf(xs_imp, y);

m_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y)

len(xs.columns), len(xs_imp.columns)

plot_fi(rf_feat_importance(m, xs_imp));

from fastbook import cluster_columns # import the function from fastbook

cluster_columns(xs_imp) # Now the function should be available

def get_oob(df):
  m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,
                            max_samples=50000, max_features=0.5,
                            oob_score=True).fit(df, y)
  return m.oob_score_

get_oob(xs_imp)

{c:get_oob(xs_imp.drop(c, axis=1)) for c in (
 'saleYear', 'saleElapsed', 'ProductGroupDesc',
 'fiModelDesc', 'fiBaseModel',
 'Hydraulics_Flow','Grouser_Tracks', 'Coupler_System')}

to_drop = ['saleYear','ProductGroupDesc','fiBaseModel','Grouser_Tracks']
get_oob(xs_imp.drop(to_drop, axis=1))

xs_final = xs_imp.drop(to_drop, axis=1)
valid_xs_final = valid_xs_imp.drop(to_drop, axis=1)

m = rf(xs_final, y)
m_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y)

p = valid_xs_final['ProductSize'].value_counts(sort=False).plot.barh()
c = to.classes['ProductSize']
plt.yticks(range(len(c)), c);

ax = valid_xs_final['YearMade'].hist()

!pip install treeinterpreter
!pip install waterfallcharts

row = valid_xs_final.iloc[:5]
row

import treeinterpreter
from treeinterpreter import treeinterpreter

prediction, bias, contributions = treeinterpreter.predict(m, row.values)

prediction[0], bias[0], contributions[0].sum()

df_dom = pd.concat([xs_final, valid_xs_final])
is_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))
m = rf(df_dom, is_valid)
rf_feat_importance(m, df_dom)[:6]

m = rf(xs_final, y)
print('orig', m_rmse(m, valid_xs_final, valid_y))
for c in ('SalesID','saleElapsed','MachineID'):
  m = rf(xs_final.drop(c,axis=1), y)
  print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y))

time_vars = ['SalesID','MachineID']
xs_final_time = xs_final.drop(time_vars, axis=1)
valid_xs_time = valid_xs_final.drop(time_vars, axis=1)
m = rf(xs_final_time, y)

m_rmse(m, valid_xs_time, valid_y)

xs['saleYear'].hist();

### removing the old data
filt = xs['saleYear']>2004
xs_filt = xs_final[filt]
y_filt = y[filt]
m = rf(xs_filt, y_filt)

### doing it using nn instead of the random forest

df_nn = pd.read_csv('/content/TrainAndValid.csv', low_memory=False)
df_nn['ProductSize'] = df_nn['ProductSize'].astype('category')
df_nn['ProductSize'].cat.set_categories(sizes, ordered=True)
df_nn[dep_var] = np.log(df_nn[dep_var])
df_nn = add_datepart(df_nn, 'saledate')

df_nn_final = df_nn[list(xs_final.columns)+[dep_var]]

cont_nn, cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)

cont_nn, cat_nn

df_nn_final[cat_nn].nunique()

xs_filt2 = xs_filt.drop('fiModelDescriptor', axis=1)
valid_xs_time2 = valid_xs_time.drop('fiModelDescriptor', axis=1)
m2 = rf(xs_filt2, y_filt)

cat_nn.remove('fiModelDescriptor')

procs_nn = [Categorify, FillMissing, Normalize]
to_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn,
 splits=splits, y_names=dep_var)

dls = to_nn.dataloaders(1024)

y = to_nn.train.y
y.min(),y.max()

learn = tabular_learner(dls, y_range=(8, 12), layers=[500, 250], n_out=1, loss_func=F.mse_loss)

min_lr = learn.lr_find()

min_lr

learn.fit_one_cycle(5, min_lr)

preds, targs = learn.get_preds()
r_mse(preds, targs)

learn.save('nn')

rf_preds = m.predict(valid_xs_final)
ens_prds = (to_np(preds.squeeze()) + rf_preds) /2

